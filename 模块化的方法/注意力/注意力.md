# 自注意力

self-attention

【基于Transformer】的架构。主要用于语言理解的建模中。
这种架构没有使用神经网络中的循环 recurrence 来学习输入和输出之间的全局依赖关系，完全依靠自注意力。

自注意力有 n 个输入 和 n 个输出。自注意力机制让每个输入都彼此交互（自），然后找到他们应该更加关注的输入（注意力）。
输出是这些交互的聚合和注意力分数。

自注意力的步骤：

1. 准备输入
2. 初始化权重
3. 推导键、查询 和 值
4. 计算输入 1 的注意力分数
5. 计算 softmax
6. 将分数与值相乘
7. 对加权的值求和
8. 为输入2和3重复 4~7步骤

Attention 的本质：__从关心全局到关心重点__

人类的视觉系统，就是一种Attention 机制。**将有限的注意力集中在重点信息上，从而节省资源，快速获得最有效的信息**

NLP > BERT、GPT > Transformer > Attention

BERT(Bidirectional Encoder Representation from Transformers)
GPT()

Attention的3大优点：
1. 参数少
2. 速度快
3. 效果好

Attention的原理
